# autonomous-driving-captioning
2025년도 하계 UROP
## 프로젝트 개요

이 프로젝트는 **자율주행 주행 장면 캡셔닝(Autonomous Driving Captioning)**을 목표로,  
멀티모달 모델인 **InstructBLIP (Vicuna-7B)**를 활용하여 이미지로부터 교통 상황을 설명하는 문장을 생성하는 연구입니다.  

- **데이터셋**: 주행 장면 이미지 3,000장 (수집 후 캡션 라벨링)  
- **환경**: Google Colab Pro+ (GPU A100)  
- **파인튜닝 방식**: LoRA
  

## 평가 방법

별도로 분리한 100장의 이미지에 대해 다음 지표로 평가를 진행했습니다:  

- **BLEU-1 ~ BLEU-4**: n-gram 기반 정확도 지표  
- **METEOR**: 의미적 매칭과 어휘 변형(동의어 등) 고려  
- **ROUGE-L**: 순서 기반 유사도(Longest Common Subsequence)  
- **BERTScore-F1**: 사전학습된 언어모델 임베딩 기반 의미적 유사도  

## 결과

| 지표          | 베이스라인 | 파인튜닝 |
|---------------|------------|-----------|
| BLEU-1        | 0.2007     | 0.3574    |
| BLEU-2        | 0.0919     | 0.2549    |
| BLEU-3        | 0.0399     | 0.1838    |
| BLEU-4        | 0.0179     | 0.1321    |
| METEOR        | 0.2633     | 0.4546    |
| ROUGE-L       | 0.2061     | 0.3738    |
| BERTScore-F1  | 0.8612     | 0.9078    |

➡️ **파인튜닝 후 모든 지표가 고르게 향상**되었으며, 특히 **BLEU와 METEOR**에서 큰 개선을 확인했습니다.  
이는 모델이 단순 키워드 매칭뿐 아니라, 문장 구조와 의미적 일관성을 더 잘 학습했음을 시사합니다.
